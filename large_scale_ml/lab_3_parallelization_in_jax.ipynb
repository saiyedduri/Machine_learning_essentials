{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3e61cf-cc72-44cc-97ce-8cba2bd43283",
   "metadata": {},
   "source": [
    "# Lab 3: Introduction to Parallelization in JAX\n",
    "---\n",
    "Author: Dr. Jan Blechschmidt\\\n",
    "Email: Jan.Blechschmidt@math.tu-freiberg.de\\\n",
    "Credits: This tutorial is based on [this tutorial](https://docs.jax.dev/en/latest/sharded-computation.html).\n",
    "\n",
    "---\n",
    "\n",
    "The following notebook is meant to give a short introduction into the three parallelization concepts in JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca1d57-b835-463a-867e-56e53d809ca6",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df430128-987d-47c0-8d96-bd07bfbf2b3e",
   "metadata": {},
   "source": [
    "In JAX, parallelization is realized by **sharding** or **partitioning** arrays across\n",
    "multiple accelerators.\n",
    "\n",
    "### Three ways to use parallelization\n",
    "\n",
    "- Automatic sharding via `jax.jit()`: The compiler chooses the optimal computation strategy, you don’t even notice that the code is executed\n",
    "on multiple devices.\n",
    "- Explicit Sharding is similar to automatic sharding in that you’re writing a\n",
    "global-view program:\n",
    "    - sharding of array’s can be explicit part of model\n",
    "    - these shardings are propagated\n",
    "    - it’s still the compiler’s responsibility to turn the whole-array program into per-device programs (turning jnp.sum into psum for example)\n",
    "    - the compiler is heavily constrained by the user-supplied shardings.\n",
    "- Manual sharding via `jax.shard_map()`: enables per-device code and explicit communication collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c6e5d-2936-409f-949f-547742e45285",
   "metadata": {},
   "source": [
    "### Sharding in JAX\n",
    "\n",
    "Key concept to all of the distributed computation approaches is **data sharding**, which describes how data is laid out on the available devices.\n",
    "\n",
    "- JAX’s datatype, the `jax.Array` immutable array data structure, represents arrays with physical  storage spanning one or multiple devices\n",
    "- this helps make parallelism a core feature of JAX\n",
    "- *every* `jax.Array` has a sharding attribute, which describes which shard of the global data is required by each global device\n",
    "- when you create a jax.Array from scratch, you can also create its\n",
    "Sharding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa93010-7000-4f5c-89d3-5c14615d0f6b",
   "metadata": {},
   "source": [
    "The function `jax.devices(backend=None)` returns a list of all available devices.\n",
    "\n",
    "If backend is `None`, it returns all the devices from the default backend.\n",
    "The default backend is generally `gpu` or `tpu` if available, otherwise `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe533e-1094-4180-9272-e3a3ae53cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad5692-5602-4ade-8721-164a55406d67",
   "metadata": {},
   "source": [
    "For an array, one can use the attribute `device` to show the corresponding device.\n",
    "Similarly, the attribute `sharding` returns the sharding of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5eaf0-c26c-4476-878d-eebb2baff790",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.arange(64.0).reshape(4,16)\n",
    "A.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b5a4f-5e3d-42e9-a82b-e2bf8fb3f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879a969-8ed5-4209-a694-3c6c4a322d45",
   "metadata": {},
   "source": [
    "Using the XLA compiler flag\n",
    "\n",
    "    xla_force_host_platform_device_count\n",
    "\n",
    "one can set the number of CPU devices visible to JAX. Note that you have to restart the notebook and either set the corresponding environment variable or setting the flag using `os.environ` **before** loading jax.\n",
    "\n",
    "If you are using Google Colab with activated TPU environment, you should already see multiple TPU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30247224-41a0-4b21-8e82-6afc98ad1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff4a136-ce24-4e5d-8de6-ce7627249991",
   "metadata": {},
   "source": [
    "The standard device is the one with `id=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ef38f-6a62-4ac5-bcc3-53213b9ccc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.arange(64.0).reshape(4,16)\n",
    "print(f'{A.device = }\\n')\n",
    "print(f'{A.sharding = }\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d0ee2-2ec6-494b-a049-51bb9d7a5a67",
   "metadata": {},
   "source": [
    "One can explicitly set the device using the optional `device` variable in functions like `jnp.Array`, `jnp.ones`, `jnp.zeros`, etc.\n",
    "\n",
    "Note the the `device`-parameter can either be `None`, `Device` or `Sharding`.\n",
    "If it is a specific `Device` or a `Sharding`, it is the one to which the created array will be committed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67c300-9402-43d9-8bc3-dcddd58e749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = jnp.ones((4,16), device=jax.devices()[1])\n",
    "print(f'{B.device = }\\n')\n",
    "print(f'{B.sharding = }\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c59ae-1d15-4c57-a081-32b84bdb7b9b",
   "metadata": {},
   "source": [
    "### Visualization of sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c7d27-ca57-4afe-8c2e-a487e1ecb49c",
   "metadata": {},
   "source": [
    "Using `jax.debug.visualize_array_sharding`, one can visualize the sharding of arrays. For the arrays `A` and `B` this is boring, since the arrays are only committed to one device.\n",
    "\n",
    "Note: Using the optional variable `scale`, one can scale the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235d0ad-d88d-411a-9176-e2a8637b0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sharding of array A:\\n')\n",
    "jax.debug.visualize_array_sharding(A, scale=0.5)\n",
    "\n",
    "print('\\nSharding of array B:\\n')\n",
    "jax.debug.visualize_array_sharding(B, scale=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f7526-c438-4b5e-b464-1138be452384",
   "metadata": {},
   "source": [
    "### Sharding over multiple devices\n",
    "\n",
    "To create an array with a non-trivial sharding, you can define a `jax.sharding` specification for the array and pass this to `jax.device_put()`. \n",
    "\n",
    "#### 1st step: Create mesh\n",
    "\n",
    "The function `jax.make_mesh` attempts to automatically compute a good\n",
    "mapping from a set of logical axes (the mesh that we want to use in our programms) to a physical mesh (important on TPUs where TPUs are aranged in a 2d torus or 3d mesh, or on GPUs, where some of the GPUs might be connected with a fast NVLink-conncetion).\n",
    "\n",
    "This finite-dimensional mesh allows to make of use specific hardware, in particular TPUs, where some of the accelators are connected with higher bandwidth than others. Thus, instead of having all devices in a one-dimensional list, we can arrange them in a multi-dimensional way, e.g., to accomodate for data and model parallelization using a `data`-axis and a `model`-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c2a34-e9d8-40a9-bc7a-afc458012502",
   "metadata": {},
   "source": [
    "The following creates a `mesh` of size 2 by 4, where the first axis is named `x` and the second dimension `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc85ec8-ad24-42a2-9157-9f94f818b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = jax.make_mesh((2, 4), ('x', 'y'))\n",
    "print(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4ce4f-c296-432e-82c0-b81cd7b2e503",
   "metadata": {},
   "source": [
    "Note here that printing reveals that the mesh axis are currently in `Auto`-mode. Later, we will learn how to work with explicit axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d1e91-3190-4819-bed0-1a31f4ec92da",
   "metadata": {},
   "source": [
    "#### 2nd step: Define a sharding\n",
    "Define a `NamedSharding`, which specifies an finite-dimensional grid of devices with *named\n",
    "axes*.\n",
    "\n",
    "A `NamedSharding` expresses a sharding using *named axes* and is a pair of a\n",
    "`mesh` and a `PartitionSpec` which describes how to shard an array across the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c89cf8-ddf8-499c-8898-b19e52b7f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec as P\n",
    "\n",
    "sharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n",
    "print(sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af3826-368f-4ea8-8d40-54be151b5b56",
   "metadata": {},
   "source": [
    "**Task**: Put the array `A` on multiple accelerators using the function `jax.device_put` and the named sharding that we just defined. Store the sharded array under `A_sharded`.\n",
    "\n",
    "Afterwards, visualize the sharding using `jax.debug.visualize_array_sharding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871b8ee-8f93-4359-bdc0-79d1edd40c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bf688-ef94-4da3-ac74-9c6ed9b60945",
   "metadata": {},
   "source": [
    "**Task**: Check the devices of `A_sharded` and the sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9e281-1ea3-4a60-952f-f889bf609e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510bb074-b4db-4a2f-91ec-ffcc280261bf",
   "metadata": {},
   "source": [
    "Note that printing the arrays `A` and `A_sharded` doesn't reveal the sharding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4f1ec-c00a-4f8a-ada4-eee6a0277cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da740751-3f8a-4d2b-b48d-121ace658ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sharded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1791581-76aa-470a-82b0-9e605d60048a",
   "metadata": {},
   "source": [
    "**Task**: Explore, what it means if you change the partition specification (`PartitionSpec`) to\n",
    "- `P('x', None)`\n",
    "- `P(None, 'y')`\n",
    "- `P(None, None)`\n",
    "- `P(None, 'x')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e21314-1715-4d81-99dc-1f5d4d5e9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b505a4e-41be-4319-85fd-4f1be661405f",
   "metadata": {},
   "source": [
    "## 1st way: Automatic parallelization using `jax.jit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f842b4d-ed71-46eb-9dc2-25303ea92fec",
   "metadata": {},
   "source": [
    "Once you have sharded data, the easiest way to do parallel computation is to simply pass the *sharded data* to a `jit`-compiled function:\n",
    "- the XLA compiler which is used during just-in-time compilation includes heuristics for optimizing computations across multiple devices\n",
    "\n",
    "- inter-device communication is done automatically\n",
    "- you can specify how you want the input and output of your code to be partitioned explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8ec85-db28-4527-84de-8c03d0a30aac",
   "metadata": {},
   "source": [
    "For simple functions, the XLA compiler heuristics result in a code where computation follows data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd584c9-3572-4e70-b29c-35dbf58321cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "  return 2 * jnp.sin(x)**2 + 1\n",
    "\n",
    "Z = f(A_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa30ba-144c-410f-9716-c14dbde8ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(Z, scale=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9c47a-7d2c-45a7-8abb-b0f5c3288c78",
   "metadata": {},
   "source": [
    "For simple function, oftentimes input sharding equals output sharding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284e606-bfdf-4cef-bc54-309a69017d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.sharding == A_sharded.sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720c5d5-59e3-47a5-9d59-ce40172bba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(A).sharding == A.sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778e255-44cd-461a-a15b-1b90cf0ff6aa",
   "metadata": {},
   "source": [
    "As computations get more complex, the compiler makes decisions about\n",
    "how to propagate the sharding of the data.\n",
    "\n",
    "Let's consider this summation over the first axis, i.e. a columnwise summation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba1a78-fde9-4e11-84e4-640bc0398043",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f_sum(x):\n",
    "  return x.sum(axis=0)\n",
    "\n",
    "Z = f_sum(A_sharded)\n",
    "\n",
    "print('Input sharding: \\n')\n",
    "jax.debug.visualize_array_sharding(A_sharded, scale=.5)\n",
    "\n",
    "print('Output sharding: \\n')\n",
    "jax.debug.visualize_array_sharding(Z, scale=.5)\n",
    "\n",
    "print(f'Output: {Z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5f2bb-0099-4dc6-bdad-8ebd7ce47def",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9be809-2804-4aa1-8ae2-766ba292f597",
   "metadata": {},
   "source": [
    "Thus, the result is partially replicated: that is, the first four elements of the\n",
    "array are replicated on devices 0 and 4, the second four on 1 and 5, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0203c-ab32-4978-a4d1-3b095441777c",
   "metadata": {},
   "source": [
    "**Task**: Create two arbitrary arrays `B` and `C` of shape 4 by 8.\n",
    "The array `B` should be sharded along the `x`-axis of the mesh and replicated along the `y`-axis.\n",
    "Array `C` should be sharded along the `y`-axis and replicated along the `x` axis.\n",
    "\n",
    "What is the sharding of `B + C`?\n",
    "\n",
    "What is the sharding of the matrix product `B.T @ C`? Think about an answer before verifying your guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876870b4-627a-4a98-8d4b-24f2826e7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c66de7-55ab-4bca-a12b-0f75882ba05a",
   "metadata": {},
   "source": [
    "### Data parallel layer of a feedforward neural network using automatic parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e20dd0-bf42-49b4-9691-93e8ce4fa1de",
   "metadata": {},
   "source": [
    "Let's consider as an example the following definition of a (vectorized) layer of a neural network with inputs:\n",
    "- `x` array of data of shape `(n_data, n_features)`\n",
    "- `W` weight matrix of shape `(n_features, n_out`)\n",
    "- `b` bias of shape `(n_out,)`\n",
    "\n",
    "Note that this layer could be the terminal layer in a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a9ab9-1e0c-473d-954d-e516db4fb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def layer(x, W, b):\n",
    "  return jax.nn.softmax(x @ W + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866ca1f-8165-4972-ba7a-2cef637a2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "n_data = 128\n",
    "n_out = 2\n",
    "\n",
    "key = jax.random.key(0)\n",
    "key, *keys = jax.random.split(key, 4)\n",
    "x = jax.random.normal(keys[0], shape=(n_data,n_features))\n",
    "W = jax.random.normal(keys[1], shape=(n_features, n_out))\n",
    "b = jax.random.normal(keys[2], shape=(n_out,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b411b0-4a84-4468-a1f2-7d5e4edd0c3f",
   "metadata": {},
   "source": [
    "Since all arrays are created without specification of a sharding, they are performed on the default device, i.e. the one with `id=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9e4bc-46bb-4784-aa5c-ff48be0de212",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = layer(x, W, b)\n",
    "print(out.shape)\n",
    "jax.debug.visualize_array_sharding(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1710c4-8780-4ebf-bd77-57c030e16f83",
   "metadata": {},
   "source": [
    "**Task**: In a full data-parallel model, we would shard the data array `x` across our devices and replicate the model parameters `W` and `b` across the devices. Implement this approach and verify that the output sharding is the same as the sharding of your data.\n",
    "\n",
    "Use a mesh with one dimension and four or eight accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131ecc9-3d3c-44ab-8707-1d7dce3e9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcaf1d-2649-4c53-a622-13b787078ba6",
   "metadata": {},
   "source": [
    "## 2nd way: Parallelization using explicit sharding\n",
    "\n",
    "- main idea is that the JAX-level type of a variable includes a description of how the variable is sharded.\n",
    "- we can query the JAX-level type of any JAX variable using `jax.typeof`\n",
    "- this also works for NumPy arrays, Python scalars and other variables\n",
    "\n",
    "Note: To use `jax.typeof`, you have to use a current version of JAX. On Google Colab, you can update jax by\n",
    "\n",
    "    !pip install jax==0.6.0\n",
    "\n",
    "A restart of the kernel is necessary.\n",
    "\n",
    "Let's consider the following examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289382f-49ac-4694-a858-c60d825bf818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_array = np.arange(8)\n",
    "print(f\"JAX-level type of Numpy array: {jax.typeof(np_array)}\")\n",
    "print(f\"JAX-level type of JAX array: {jax.typeof(A)}\")\n",
    "print(f\"JAX-level type of Python scalar: {jax.typeof(4.2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08478655-5a24-4d2b-9d17-c09a11818fb8",
   "metadata": {},
   "source": [
    "Thus, one can think of the JAX-level type to be the information about a value\n",
    "within just-in-time compilation.\n",
    "It is even possible to get the JAX-level type during tracing of a jit-compiled function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ba620-4997-4878-99a1-980f2c95a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def g(x):\n",
    "  print(f\"JAX-level type of x during tracing: {jax.typeof(x)}\")\n",
    "  return x\n",
    "g(A);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d673d36-6fff-45df-9c73-08f4dbf5ad0e",
   "metadata": {},
   "source": [
    "Note that the JAX-level type of a sharded array still reveals it's global type, e.g. in the case of `A_sharded` the type `ShapedArray(float32[4,16])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3c665-80c1-466a-89dd-55898d3520db",
   "metadata": {},
   "outputs": [],
   "source": [
    "g(A_sharded);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf640c9-3585-47fa-b483-2c2d764a2de1",
   "metadata": {},
   "source": [
    "To make use of explicit sharding, we first have to create a mesh with *explicit axes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4c556-9f4e-44cf-b9bf-ac72e59ad7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import AxisType\n",
    "\n",
    "mesh = jax.make_mesh((2, 4), (\"x\", \"y\"),\n",
    "                     axis_types=(AxisType.Explicit, AxisType.Explicit))\n",
    "print(mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672ff4c-973b-4539-a94d-39d35b03621b",
   "metadata": {},
   "source": [
    "As the print reveals, the `axis_types` are now `Explicit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c3f6c9-b905-4f6b-9c4a-0deebe707ace",
   "metadata": {},
   "source": [
    "We now create a new array `A` of shape 8 by 4 and shard it using our mesh with explicit axes and in a way, such that it is sharded along the `x`-axis and replicated along the `y`-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d0091-4dae-4bbe-afe1-efc93f2d2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(32.).reshape(8, 4)\n",
    "A_sharded = jax.device_put(A, jax.NamedSharding(mesh, P(\"x\", None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01563a88-c618-4f71-8749-bbaf7b57e96e",
   "metadata": {},
   "source": [
    "Let's take a look the the JAX-level types of `A` and `A_sharded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e297c-3a7e-40be-a7dc-b6314a1aec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Type of A: {jax.typeof(A)}\")\n",
    "print(f\"Type of A_sharded: {jax.typeof(A_sharded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9142a9e-bee3-4ebd-94f3-b8eb5e5770b5",
   "metadata": {},
   "source": [
    "One can read the type `float32[8@x, 4]` as an 8-by-4 array of 32-bit floats whose first dimension is sharded along mesh axis `x`. The array is replicated along the other mesh axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbb57d-8443-4bb3-b141-54ec04ba89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(A_sharded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b29ae-6693-41c1-a219-12824babbefa",
   "metadata": {},
   "source": [
    "The next cell defines two sharded arrays, the first one is sharded along the `x` axis and replicated along `y`, the second one is replicated along the `x` axis and sharded along the `y` axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794eebf-1710-46ff-9e46-2e275e00cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jax.device_put(np.arange(4).reshape(4, 1),\n",
    "                   jax.NamedSharding(mesh, P(\"x\", None)))\n",
    "B = jax.device_put(np.arange(8).reshape(1, 8),\n",
    "                   jax.NamedSharding(mesh, P(None, \"y\")))\n",
    "\n",
    "print('Sharding of A:\\n')\n",
    "jax.debug.visualize_array_sharding(A)\n",
    "\n",
    "print('\\nSharding of B:\\n')\n",
    "jax.debug.visualize_array_sharding(B, scale=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc82333-3ed1-4736-b9fb-c1fef6f74541",
   "metadata": {},
   "source": [
    "The next cell implements a function which just performs an elementwise multiplication (using the `*` operator) and prints the JAX-level types of the inputs and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49037c6d-94cd-4e06-8e69-397c80cd8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def multiply_arrays(x, y):\n",
    "  ans = x * y\n",
    "  print(f\"x sharding: {jax.typeof(x)}\")\n",
    "  print(f\"y sharding: {jax.typeof(y)}\")\n",
    "  print(f\"ans sharding: {jax.typeof(ans)}\")\n",
    "  return ans\n",
    "\n",
    "C = multiply_arrays(A, B)\n",
    "\n",
    "jax.debug.visualize_array_sharding(C, scale=0.4)\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf7753-5236-49b3-ad11-45851ea0cd9b",
   "metadata": {},
   "source": [
    "**Note**. Shardings propagate deterministically at trace time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304149b2-e7e6-4ab1-98bb-11c2727020df",
   "metadata": {},
   "source": [
    "### Data parallel layer of a feedforward neural network using explicit sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f3cf3-1c25-4751-930c-02b022263805",
   "metadata": {},
   "source": [
    "Below, you find again the example of one layer of a feedforward neural network from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719cb52-e53a-40c8-9f0b-ba80bb876e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def layer(x, W, b):\n",
    "  return jax.nn.softmax(x @ W + b)\n",
    "    \n",
    "n_features = 10\n",
    "n_data = 128\n",
    "n_out = 2\n",
    "\n",
    "key = jax.random.key(0)\n",
    "key, *keys = jax.random.split(key, 4)\n",
    "x = jax.random.normal(keys[0], shape=(n_data,n_features))\n",
    "W = jax.random.normal(keys[1], shape=(n_features, n_out))\n",
    "b = jax.random.normal(keys[2], shape=(n_out,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5721424-d367-4449-b725-f1c4b517184e",
   "metadata": {},
   "source": [
    "**Task**: Implement a data parallel execution of the layer using **explicit sharding** mode. Use again a mesh with one dimension and four or eight accelerators.\n",
    "\n",
    "Write a wrapper-function which executes the layer function but also prints the `JAX`-level types during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9d0ea-3411-4611-ade2-4b49e3626330",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9e80b-ad89-4951-8add-92eed766541e",
   "metadata": {},
   "source": [
    "## 3rd way: Manual parallelization\n",
    "\n",
    "The first two approaches (automatic parallelization and explicit sharding) deal with the case, where we write a function as if we are operating on the full dataset and `jax.jit` will split that computation across multiple devices.\n",
    "\n",
    "The 3rd way uses manual parallelization:\n",
    "- Here, we write a function that will handle a single shard of data, and `jax.shard_map()` will construct the full function $\\leadsto$ it thus gives the most flexibility about parallelization and communication\n",
    "- `shard_map()` works by mapping a function across a particular mesh of devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15b555-b474-4cfb-8850-9801d015c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only necessary for older versions\n",
    "from jax.experimental.shard_map import shard_map\n",
    "# otherwise use\n",
    "#from jax import shard_map\n",
    "\n",
    "mesh = jax.make_mesh((4,), ('x',))\n",
    "\n",
    "@jax.jit\n",
    "def f(x):\n",
    "    return 2*jnp.sin(x)**2 + 1\n",
    "    \n",
    "f_sharded = shard_map(\n",
    "    f,\n",
    "    mesh=mesh,\n",
    "    in_specs=P('x'),\n",
    "    out_specs=P('x'))\n",
    "\n",
    "A = jnp.arange(32.)\n",
    "Z = f_sharded(A)\n",
    "\n",
    "print('Sharding of f_sharded(A):\\n')\n",
    "jax.debug.visualize_array_sharding(Z, scale=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ea22c-ceca-4eea-a335-81cb8c91d4e9",
   "metadata": {},
   "source": [
    "Note the following: \n",
    "- `jax.sharding.Mesh` allows for precise device placement\n",
    "- the `in_specs` argument determines the shard sizes of the input\n",
    "- the `out_specs` argument identifies how the blocks are assembled back together\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d1323-f515-467f-a334-0670647fa16d",
   "metadata": {},
   "source": [
    "The function which is sharded by `shard_map` only considers a single batch of the data, which you\n",
    "can check by printing the device local shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d366d9f-5aa7-4dc6-8509-5954604d675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.arange(32)\n",
    "print(f\"global shape: {x.shape=}\")\n",
    "\n",
    "def f(x):\n",
    "    print(f\"Shape of x on local device: {x.shape}\")\n",
    "    print('\\nValue of x on local device: ', x)\n",
    "    print('\\nType of x: ', jax.typeof(x))\n",
    "    return x * 2\n",
    "\n",
    "f_sharded = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))\n",
    "y = f_sharded(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e9196-dfd8-43c1-ba3e-6c48efd131fd",
   "metadata": {},
   "source": [
    "Here, you can see the explicit shards during execution of the function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec62440-4762-4052-b709-943ae6279a0c",
   "metadata": {},
   "source": [
    "Because each function only sees the device-local part of the data, it means\n",
    "that aggregation-like functions don’t work as expected automatically.\n",
    "\n",
    "See for example the function which sums all elements of an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec19fee-9696-435b-abbd-cd120851e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  return jnp.sum(x, keepdims=True)\n",
    "\n",
    "f_sharded = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))\n",
    "f_sharded(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41830bb9-9be2-48d7-a3ef-68d95684fbd1",
   "metadata": {},
   "source": [
    "Here, you see that the function operates separately on each shard, and the resulting\n",
    "summation reflects this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6621a-2585-4808-b835-7d7a6398c86d",
   "metadata": {},
   "source": [
    "If you want to sum across shards instead, you need to explicitly request it suing collective operations like `jax.lax.psum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92264932-0fcd-4b9c-81b5-7ddb85f9dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  local_sum = x.sum()\n",
    "  return jax.lax.psum(local_sum, 'x')\n",
    "\n",
    "f_sharded = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P())\n",
    "f_sharded(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9c739-c638-4a71-8c6e-1bcb4e4512c7",
   "metadata": {},
   "source": [
    "Note also, that the output has no longer a sharded dimension, since we specified `out_specs=P()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1940f-3b0a-41b8-bedc-fac3ca8a156b",
   "metadata": {},
   "source": [
    "### Data parallel layer of a feedforward neural network using `shard_map`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef184034-8bef-4a08-a90c-ce6879fa171a",
   "metadata": {},
   "source": [
    "Below, you find again the example of one layer of a feedforward neural network from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65770af2-2055-499d-8a18-260be0b293af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def layer(x, W, b):\n",
    "  return jax.nn.softmax(x @ W + b)\n",
    "    \n",
    "n_features = 10\n",
    "n_data = 128\n",
    "n_out = 2\n",
    "\n",
    "key = jax.random.key(0)\n",
    "key, *keys = jax.random.split(key, 4)\n",
    "x = jax.random.normal(keys[0], shape=(n_data,n_features))\n",
    "W = jax.random.normal(keys[1], shape=(n_features, n_out))\n",
    "b = jax.random.normal(keys[2], shape=(n_out,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdcd43-9621-4b61-8d9a-a554730c0d7b",
   "metadata": {},
   "source": [
    "**Task**: Implement a data parallel execution of the layer using manual parallelization through `shard_map`. Use a mesh with one dimension and four or eight accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47c4b5-2fa6-4bf8-af86-e45a7c2a1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546f39b-b854-4725-a528-2c0e1c623f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_man.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52966e-cc1a-4230-9725-2c26e75d6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(out_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac21f6-ff87-4549-a9de-e070ce17ea52",
   "metadata": {},
   "source": [
    "## Neural networks using automatic parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a2d85-f5bb-4ee1-97da-ab4c08708f74",
   "metadata": {},
   "source": [
    "The following implements a feedforward neural network that can be used for solving regression problems. To see the expected speedup of parallelization, it should be executed on a machine with either multiple TPUs or GPUs.\n",
    "This is due to the fact that the XLA compiler in `jax.jit` already parallelizes such simple programs automatically quite well and uses all CPU resources available.\n",
    "You can verify this by taking a look at some resource monitor (e.g. `htop`) while the computations run.\n",
    "\n",
    "Note however, that you can still implement the data parallelization as requested in the Task below. We are using a similar feedforward network model as used in Lab 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f6a5d9-35a1-4156-8d2b-b86130c4aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "from jax.sharding import PartitionSpec as P\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define population line\n",
    "def f(x):\n",
    "    return jnp.sin(6*x) - .5 * x**2\n",
    "\n",
    "def gen_data(key, f, n=100, sigma=0.1, xmin=0.0, xmax=2.0):\n",
    "    keys = jax.random.split(key, 2)\n",
    "    x = jax.random.uniform(keys[0], shape=(n, 1), minval=xmin, maxval=xmax)\n",
    "    y = f(x) + sigma * jax.random.normal(keys[1], shape=(n, 1))\n",
    "    return x, y\n",
    "\n",
    "def random_layer_params(n_in, n_out, key, scale=1e-1):\n",
    "    w_key, b_key = jax.random.split(key)\n",
    "    # Weight matrix\n",
    "    w = scale * jax.random.normal(w_key, (n_in, n_out))\n",
    "    # Bias vector\n",
    "    b = scale * jax.random.normal(b_key, (n_out,))\n",
    "    return w, b\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    keys = jax.random.split(key, len(sizes) - 1)\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "def forward(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(x, w) + b\n",
    "        x = jnp.tanh(outputs)\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    out = jnp.dot(x, final_w) + final_b\n",
    "    return out\n",
    "    \n",
    "def loss(params, x, y):\n",
    "    return jnp.mean(jnp.square(forward(params, x) - y))\n",
    "    \n",
    "grad_loss = jax.jit(jax.grad(loss, argnums=0))\n",
    "\n",
    "def train(params, x, y, epochs=100, step_size=1e-3):\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        grads = grad_loss(params, x, y)\n",
    "        params = [(W - step_size * dW, b - step_size * db)\n",
    "                  for (W, b), (dW, db) in zip(params, grads)]\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882a330-ec7c-4bd9-860d-48d25113cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_key = jax.random.key(seed=0)\n",
    "gen_key, *keys = jax.random.split(global_key,3)\n",
    "\n",
    "if jax.devices()[0].device_kind == 'cpu':\n",
    "    n = 1024\n",
    "    layer_sizes = [1, 1024, 1024, 1]\n",
    "else:\n",
    "    n = 8192*2\n",
    "    layer_sizes = [1, 8192, 8192, 8192, 1]\n",
    "\n",
    "x, y = gen_data(keys[0], f, n=n)\n",
    "params = init_network_params(layer_sizes, keys[1])\n",
    "\n",
    "# Execute grad_loss once to compile function\n",
    "grad_loss(params, x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8f237-aea6-4cb2-9b98-5099f5217cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_single = train(params, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e4b25-dcf3-4c85-a8a9-8491c8de923b",
   "metadata": {},
   "source": [
    "**Task**: Implement the training step using automatic parallelization. The training data `(x,y)` should be sharded across all devices, the parameters `(w, b)` should be replicated. Finally, train the model using your sharded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce662dce-b070-4917-90c3-ac5cd8ec4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e07cc-4113-499b-9578-981d157889ed",
   "metadata": {},
   "source": [
    "**Task**: Compare the runtimes of the function `grad_loss` using both the sharded and unsharded data. On a machine with 8 TPUs, like the one that is available on Google Colab for free, you should observe a speedup factor of around 6 to 8.\n",
    "\n",
    "On a CPU, the implemention that uses your parallelization takes probably longer due to the automatic parallelization and optimizations of the XLA compiler when applied to the unsharded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eba49d-7968-4e99-8e68-a2819385221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
