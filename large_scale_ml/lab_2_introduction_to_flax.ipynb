{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lab 2: Introduction to Flax\n",
    "---\n",
    "Author: Dr. Jan Blechschmidt\\\n",
    "Email: Jan.Blechschmidt@math.tu-freiberg.de\\\n",
    "Credits: This tutorial is mainly based on [this tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html) and the [tutorial on Flax](https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html).\n",
    "\n",
    "---\n",
    "\n",
    "The following notebook is meant to give a short introduction writing and training your own neural networks with JAX and [Flax](https://flax.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Implementing a Neural Network for 1D regression using Jax\n",
    "\n",
    "With having reviewed the basics of JAX in the last lab and learned the basics of neural networks and backpropagation in the last lecture, we are now ready to implement our own first neural network from scratch in JAX.\n",
    "\n",
    "In this section, we implement a simple neural network using mainly JAX. Later, we will use the module **FLAX**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the true function \n",
    "\n",
    "$$\n",
    "f(x) = \\sin(6\\,x) - 0.5 x^2\n",
    "$$\n",
    "\n",
    "which is unknown in training and sometimes called **population line**.\n",
    "\n",
    "**Task**. Implement the function using `jax.numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function draws random data pairs $\\{(x_i,y_i)\\}_{i=1}^n$. Currently, the implementation draws exact values, i.e.\n",
    "\n",
    "$$\n",
    "y_i = f(x_i).\n",
    "$$\n",
    "\n",
    "However, in real situation we oftentimes observe only noisy data.\n",
    "\n",
    "**Task**. Adjust the code such that the function returns noisy data\n",
    "\n",
    "$$\n",
    "y_i = f(x_i) + \\sigma \\, Z_i \\quad Z_i \\sim \\mathcal N (0,1)\n",
    "$$\n",
    "\n",
    "instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(key, f, n=100, sigma=0.1, xmin=0.0, xmax=2.0):\n",
    "    x = jax.random.uniform(key, shape=(n, 1), minval=xmin, maxval=xmax)\n",
    "    y = f(x)\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you updated the code correctly, the next cell should illustrate this one-dimensional dataset with noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_key = jax.random.key(seed=0)\n",
    "gen_key, global_key = jax.random.split(global_key)\n",
    "\n",
    "ntrain, ntest = 100, 20\n",
    "x, y = gen_data(gen_key, f, n= ntrain + ntest)\n",
    "xtrain, xtest = x[:ntrain], x[ntrain:]\n",
    "ytrain, ytest = y[:ntrain], y[ntrain:]\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(xtrain, ytrain, 'b+', label='train data')\n",
    "plt.plot(xtest, ytest, 'r+', label='test data')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a simple feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**. Implement a simple neural network with one input neuron, two hidden layers with 10 neurons each and one output neuron. One can abbreviate such an architecture using a list\n",
    "\n",
    "    layer_sizes = [1, 10, 10, 1]\n",
    "\n",
    "Set up a function which generates appropriate weight matrices and bias vectors using randomly initialized weights.\n",
    "You can store the weight matrices and bias vectors in a list of tuples, e.g.\n",
    "\n",
    "    params = [(W_1, b_1), (W_2, b_2), (W_3, b_3)]\n",
    "\n",
    "but other structures should also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [1, 10, 10, 1]\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Define a function `forward(params, x)` which performs the forward pass through the network and use the hyperbolic tangent function as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test forward pass on single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = forward(params, xtrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check, if this works for multiple samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  preds = forward(params, xtrain)\n",
    "except TypeError:\n",
    "  print('Invalid shapes!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**. If it's not working for batches of data, i.e. multiple samples at once, fix this using `jax.vmap` and save the *vectorized* function as `vforward`.\n",
    "Note that there should be no vectorization for the first input (`params`).\n",
    "The vectorization in the second input should be in the usual *batch dimension* zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap, jit, grad\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**. Implement the mean-squared error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, x, y):\n",
    "    ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the gradient of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Use the Jax function `grad` to derive the gradient of the loss function w.r.t. the parameters of the neural network.\n",
    "Test the function on the training data set `(xtrain, ytrain)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function implements one update step of the gradient descent algorithm with a fixed step size. Depending on your implementation, you might have to update the function slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params, x, y, step_size):\n",
    "  grads = grad_loss(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following cell trains the model for 10.000 epochs using a gradient descent algorithm with constant step size. Note that we compute always the full gradient using all training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.1\n",
    "num_epochs = 10000\n",
    "\n",
    "start_time = time()\n",
    "print(f\" Epoch     |    Train loss    |    Test loss   \")\n",
    "for epoch in range(num_epochs):\n",
    "    params = update(params, x, y, step_size=step_size)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        \n",
    "        train_loss = loss(params, xtrain, ytrain)\n",
    "        test_loss = loss(params, xtest, ytest)\n",
    "        print(f\"  {epoch:6d}   |     {train_loss:6.2e}     |     {test_loss:6.2e}\")\n",
    "        \n",
    "epoch_time = time() - start_time\n",
    "\n",
    "print(f\"Training time for {num_epochs} epochs: {epoch_time:0.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**. Plot the original data set (can be copied from above) together with the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Extension to a two-dimensional example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell generates a new set of data where the features (aka $x$ values) are two dimensional, the predictor variables (aka $y$ values) are again scalar-valued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_key = jax.random.key(seed=0)\n",
    "gen_key, global_key = jax.random.split(global_key)\n",
    "\n",
    "def f(x):\n",
    "    x0 = x[:, [0]]\n",
    "    x1 = x[:, [1]]\n",
    "    return jnp.sin(6 * x0) * jnp.cos(2 * x1) - 0.5 * x0**2 + 0.1 * x0 * x1\n",
    "    \n",
    "def gen_2d_data(key, n=100, sigma=0.1, xmin=0.0, xmax=2.0):\n",
    "    keys = jax.random.split(key, 2)\n",
    "    x = jax.random.uniform(keys[0], shape=(n, 2), minval=xmin, maxval=xmax)\n",
    "    y = f(x) + sigma * jax.random.normal(keys[1], shape=(n, 1))\n",
    "    return x, y\n",
    "    \n",
    "ntrain, ntest = 100, 20\n",
    "x, y = gen_2d_data(gen_key, n= ntrain + ntest)\n",
    "xtrain, xtest = x[:ntrain], x[ntrain:]\n",
    "ytrain, ytest = y[:ntrain], y[ntrain:]\n",
    "\n",
    "ax = plt.figure(3).add_subplot(projection='3d')\n",
    "ax.scatter(*xtrain.T, ytrain[:, 0], label='train data')\n",
    "ax.scatter(*xtest.T, ytest[:, 0], label='test data')\n",
    "ax.set_xlabel('$x_0$'), ax.set_ylabel('$x_1$'), ax.set_zlabel('$y$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Adapt the code for the one-dimensional example from above by copying and modifying the parts that have to be changed.\n",
    "The goal is to solve the regression problem on this dataset with a test error below $10^{-2}$.\n",
    "The training should take at most 30.000 epochs. You are free to change the activation function, step size and network architecture\n",
    "\n",
    "Think carefully which parts of the code have to be copied and modified! You shouldn't need much if you implemented the functions above independently of the actual layer sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Training of the network for 20000 epochs with fixed step size\n",
    "step_size = 0.1\n",
    "num_epochs = 20000\n",
    "\n",
    "start_time = time()\n",
    "print(f\" Epoch     |    Train loss    |    Test loss   \")\n",
    "for epoch in range(num_epochs):\n",
    "    params = update(params, x, y, step_size=step_size)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        \n",
    "        train_loss = loss(params, xtrain, ytrain)\n",
    "        test_loss = loss(params, xtest, ytest)\n",
    "        print(f\"  {epoch:6d}   |     {train_loss:6.2e}     |     {test_loss:6.2e}\")\n",
    "        \n",
    "epoch_time = time() - start_time\n",
    "\n",
    "print(f\"Training time for {num_epochs} epochs: {epoch_time:0.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Illustrate your learned regression function together with the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(4).add_subplot(projection='3d')\n",
    "ax.scatter(*xtrain.T, ytrain[:, 0], label='train data')\n",
    "ax.scatter(*xtest.T, ytest[:, 0], label='test data')\n",
    "ax.set_xlabel('$x_0$'), ax.set_ylabel('$x_1$'), ax.set_zlabel('$y$')\n",
    "plt.legend();\n",
    "\n",
    "X, Y = jnp.meshgrid(xlin.squeeze(), xlin.squeeze())\n",
    "XY = jnp.hstack([X.reshape((-1, 1)), Y.reshape((-1, 1))])\n",
    "### YOUR CODE HERE\n",
    "# Define Z as the evaluation of your neural network model at points XY\n",
    "# and reshape it appropriately\n",
    "ax.plot_surface(X, Y, Z, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Implementing a Neural Network for classification with Flax\n",
    "\n",
    "We have now implemented our own neural network from scratch with JAX.\n",
    "Of course, we do not really want to do that every time. Similarly to PyTorch's `torch.nn` package, there exist neural network libraries based on JAX which provide such basic functionality. A (non-exclusive) collection of them are:\n",
    "\n",
    "* [Flax](https://flax.readthedocs.io/en/latest/index.html), started by the Google Brain Team, focuses on flexibility and clarity.\n",
    "* [Haiku](https://dm-haiku.readthedocs.io/en/latest/), from DeepMind, focuses on simplicity and compositionality.\n",
    "* [Trax](https://github.com/google/trax), maintained by the Google Brain Team, provides solutions for common training tasks\n",
    "* [Equinox](https://github.com/patrick-kidger/equinox), created by Patrick Kidger and Cristian Garcia, implements neural networks as callable PyTrees\n",
    "* [Jraph](https://github.com/deepmind/jraph), from DeepMind, is a graph neural network library (similar to PyTorch Geometric)\n",
    "\n",
    "For this tutorial series, we will use **Flax** due to its flexibility, intuitive API, and large community. However, this should not mean that the other libraries are necessarily worse, and we recommend giving them a try as well to find the best library for yourself!\n",
    "\n",
    "We will introduce the libraries and all additional parts you might need to train a neural network in Flax, using a simple example classifier on a simple yet well known example: XOR.\n",
    "\n",
    "Given two binary inputs $x_1$ and $x_2$, the label to predict is $1$ if either $x_1$ or $x_2$ is $1$ while the other is $0$, or the label is $0$ in all other cases. The example became famous by the fact that a single neuron, i.e. a linear classifier, cannot learn this simple function. Hence, we will learn how to build a small neural network that can learn this function. \n",
    "To make it a little bit more interesting, we move the XOR into continuous space and introduce some Gaussian noise on the binary inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "The package `flax.linen` defines a series of useful classes like standard linear network layers, convolutions, activation functions etc.\n",
    "A full list can be found [here](https://flax.readthedocs.io/en/latest/flax.linen.html).\n",
    "In case you need a certain network layer, check the documentation of the package first before writing the layer yourself as the package likely contains the code for it already. We import it below.\n",
    "If the import fails, you can install it using \n",
    "\n",
    "    !pip install --quiet flax\n",
    "\n",
    "directly from this notebook. This should also work on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linen is a neural network API and is implemented as a submodule in flax. Importing this submodule as `nn` is standard and you will find many examples which contain this line.\n",
    "Linen builds on a \"functional core\" which enables direct usage of JAX transformations such as vmap, remat or scan inside your functions\n",
    "The Linen Module API is stable and currently recommended for new projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The basic class: `nn.Module`\n",
    "\n",
    "Similar to PyTorch, a neural network is built up out of modules. Linen modules can contain other modules, and a neural network is considered to be a module itself as well. The basic template of a module is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    # Some dataclass attributes, like hidden dimension, number of layers, etc. of the form:\n",
    "    # varname : vartype\n",
    "        \n",
    "    def setup(self):\n",
    "        # Flax uses \"lazy\" initialization. This function is called once before you\n",
    "        # call the model, or try to access attributes. In here, define your submodules etc.\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # Function for performing the calculation of the module.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main, obvious difference to PyTorch is that Flax uses lazy initialization:\n",
    "\n",
    "The function `setup` is called once on a module instance before any other methods are called, or when you try to access a attribute of `self` defined in `setup`.\n",
    "\n",
    "Other important points are:\n",
    "- Additional object attributes are defined below the class name.\n",
    "- In contrast to PyTorch, the parameters are *not* part of the module.\n",
    "- Instead, we can create a set of parameters of the module by calling its `init()` function. This function takes as input a **PRNG state** for sampling pseudo-random numbers and an example input to the model, and returns a set of parameters for the module as a **pytree**.\n",
    "- Further, since the `init` function requires an input to the network, we can infer the input shape for all modules and do not need to explicitly define it during the module creation.\n",
    "- The `__call__` method represents the `forward` function in PyTorch, and performs the actual computation of the module. It can take additional arguments if needed, like whether we are training or not, which is important in some case, e.g. if your model relies on Dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple classifier\n",
    "\n",
    "To get an intuition behind how we work with modules in Flax, let's define our own small neural network. We will use a minimal network with a input layer, one hidden layer with tanh as activation function, and a output layer. In other words, our networks should look something like this:\n",
    "\n",
    "<center width=\"100%\"><img src=\"./fig_FNN.png\" alt=\"Feedforward network\" width=\"300px\"></center>\n",
    "The input neurons are shown in yellow (we need only two of them), which represent the coordinates $x_1$ and $x_2$ of a data point. The hidden neurons including a tanh activation are shown in blue and green, and the output neuron in red.\n",
    "\n",
    "In Flax, we can define this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    num_hidden : int   # Number of hidden neurons\n",
    "    num_outputs : int  # Number of output neurons\n",
    "\n",
    "    def setup(self):\n",
    "        # Create the modules we need to build the network\n",
    "        # nn.Dense is a linear layer\n",
    "        self.linear1 = nn.Dense(features=self.num_hidden)\n",
    "        self.linear2 = nn.Dense(features=self.num_outputs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        x = self.linear1(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that you should observe that we do not define any input dimension since this in inferred during initialization through an example input.\n",
    "\n",
    "Another thing that you may notice is that usually, all layers that we define in `setup` are also used in the `__call__` function. To reduce the code overhead, Flax provides an alternative, more compact network creation with the `nn.compact` decorator (recall that you've already learned about the `jax.jit` decorator). With that, we can remove the setup function and instead our model as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifierCompact(nn.Module):\n",
    "    num_hidden : int   # Number of hidden neurons\n",
    "    num_outputs : int  # Number of output neurons\n",
    "\n",
    "    @nn.compact  # Tells Flax to look for defined submodules\n",
    "    def __call__(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        # while defining necessary layers\n",
    "        x = nn.Dense(features=self.num_hidden)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(features=self.num_outputs)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.compact` annotation of the `__call__` method signals Flax to look for submodules that we define in the forward pass. These are automatically recognized as such, so that we can use them for initialization etc. Which of the two model definition you use is often up to you (see the [Flax documentation](https://flax.readthedocs.io/en/latest/design_notes/setup_or_nncompact.html) for some pros and cons for both methods).\n",
    "\n",
    "As a rule of thumb, use the compact version where possible, and come back to the explicit setup function where necessary.\n",
    "For instance, if we define more functions on a module besides `__call__` and want to reuse some modules, it is recommended to use the setup version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the examples in this notebook, we will use a tiny neural network with two input neurons and eight hidden neurons. As we perform binary classification, we will use a single output neuron. Note that we do not apply a sigmoid on the output yet. This is because other functions, especially the loss, are more efficient and precise to calculate on the original outputs instead of the sigmoid output. We will discuss the detailed reason later.\n",
    "\n",
    "Now, let's create an instance of this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassifier(num_hidden=8, num_outputs=1)\n",
    "# Printing the model shows its attributes\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the model has no parameters initialized. To do this, let's create a random input of our dataset, and apply the init function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new pseudo random number generator (PRNG)\n",
    "rng = jax.random.key(seed=0)\n",
    "\n",
    "# As usual, we split the PRNG\n",
    "rng, inp_rng, init_rng = jax.random.split(rng, 3)\n",
    "\n",
    "# Generate a sample input of batch size 8 and input size 2\n",
    "inp = jax.random.normal(inp_rng, (8, 2))\n",
    "\n",
    "# Initialize the model\n",
    "params = model.init(init_rng, inp)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can use a `tree.map` function to print the shape of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `model.tabulate` can be used to get a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tabulate(rng, inp, console_kwargs={\"force_jupyter\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have parameters with which we can apply the network. We see that the parameters follow the same structure as defined in our module, and each linear layer contains one `kernel`, i.e. the weights, and a bias parameter. With this, we could apply the model on an input using the `apply` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(params, inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns an output array of shape `[8,1]`, which corresponds to the one output neuron in the model for all 8 batch elements. With that, we now know how to initialize a model, and run a model. Next, let's look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "As mentioned before, JAX is not meant to 'reinvent the wheel' for every part of the deep learning pipeline. Hence, JAX and Flax do not natively provide a data loading functionality, but instead refer to other available libraries like Tensorflow and PyTorch. Here, let's use the package `torch.utils.data` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data package defines two classes which are the standard interface for handling data in PyTorch: `data.Dataset`, and `data.DataLoader`. The dataset class provides a uniform interface to access the training/test data, while the data loader makes sure to efficiently load and stack the data points from the dataset into batches during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset class\n",
    "\n",
    "The dataset class summarizes the basic functionality of a dataset in a natural way. To define a dataset in PyTorch, we simply specify two functions: `__getitem__`, and `__len__`. The get-item function has to return the $i$-th data point in the dataset, while the len function returns the size of the dataset. For the XOR dataset, we can define the dataset class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class XORDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, size, seed, std=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            size - Number of data points we want to generate\n",
    "            seed - The seed to use to create the PRNG state with which we want to generate the data points\n",
    "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.np_rng = np.random.RandomState(seed=seed)\n",
    "        self.std = std\n",
    "        self.generate_continuous_xor()\n",
    "\n",
    "    def generate_continuous_xor(self):\n",
    "        # Each data point in the XOR dataset has two variables, x0 and x1, that can be either 0 or 1\n",
    "        # The label is their XOR combination, i.e. 1 if only x0 or only x1 is 1 while the other is 0.\n",
    "        # If x0=x1, the label is 0.\n",
    "        x = self.np_rng.randint(low=0, high=2, size=(self.size, 2)).astype(np.float32)\n",
    "        y = (x.sum(axis=1) == 1).astype(np.int32)\n",
    "        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n",
    "        x += self.np_rng.normal(loc=0.0, scale=self.std, size=x.shape)\n",
    "\n",
    "        self.x = x # Data\n",
    "        self.y = y # Label\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.x[idx]\n",
    "        data_label = self.y[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use NumPy to generate the random data. Similar to JAX, NumPy also allows the pseudo-number generation based on a PRNG state. Hence, for better reproducibility, we are doing the same here. Let's try to create such a dataset and inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = XORDataset(size=200, seed=42)\n",
    "print(\"Size of dataset:\", len(dataset))\n",
    "print(\"Data point 0:\", dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better relate to the dataset, we visualize the samples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(data, label):\n",
    "    data_0 = data[label == 0]\n",
    "    data_1 = data[label == 1]\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
    "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
    "    plt.title(\"Dataset samples\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(dataset.x, dataset.y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data loader class\n",
    "\n",
    "The advantages of using the `torch` data loader class are manifold. The class `torch.utils.data.DataLoader` represents a Python iterable over a dataset with support for **automatic batching**, **multi-process data loading** and many more features.\n",
    "\n",
    "The data loader communicates with the dataset using the function `__getitem__`, and stacks its outputs as tensors over the first dimension to form a batch.\n",
    "It is a convention that the first dimension is reserved as the *batch dimension*.\n",
    "\n",
    "In contrast to the dataset class, we usually don't have to define our own data loader class, but can just create an object of it with the dataset as input. Additionally, we can configure our data loader with the following input arguments:\n",
    "\n",
    "* `batch_size`: Number of samples to stack per batch\n",
    "* `shuffle`: If True, the data is returned in a random order. This is important during training for introducing stochasticity. \n",
    "* `num_workers`: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.\n",
    "* `persistent_workers`: If True, workers will not be shutdown after an iteration over the dataset has finished. This can be useful if the time per epoch is small, or if you face issues with workers being killed during training. \n",
    "* `drop_last`: If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size.\n",
    "* `collate_fn`: A function that defines how the elements per batch are combined. By default, PyTorch stacks them as PyTorch tensors. For JAX, we will change it to NumPy arrays.\n",
    "\n",
    "There are many more options which you can find [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Let's create a simple data loader below with a function that stacks batch elements as a NumPy array instead of PyTorch Tensors. Note that one can also stack the elements as JAX arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This collate function is taken from the JAX tutorial with PyTorch Data Loading\n",
    "# https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html\n",
    "\n",
    "def numpy_collate(batch):\n",
    "  \"\"\"\n",
    "  Collate function specifies how to combine a list of data samples into a batch.\n",
    "  default_collate creates pytorch tensors, then tree_map converts them into numpy arrays.\n",
    "  \"\"\"\n",
    "  return jax.tree.map(np.asarray, data.default_collate(batch))\n",
    "    \n",
    "data_loader = data.DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, the Pytorch data loader class becomes very handy for more complex data sets.\n",
    "The call \n",
    "\n",
    "    next(iter(...))\n",
    "\n",
    "returns the first batch of the data loader, if `shuffle` is `True`, this will return a different batch every time we run this cell\n",
    "For iterating over the whole dataset, we can simple use \n",
    "\n",
    "    for batch in data_loader: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs, data_labels = next(iter(data_loader))\n",
    "\n",
    "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the \n",
    "# dimensions of the data point returned from the dataset class\n",
    "print(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\n",
    "print(\"Data labels\", data_labels.shape, \"\\n\", data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "After defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:\n",
    "\n",
    "1. Get a batch from the data loader\n",
    "2. Obtain the predictions from the model for the batch\n",
    "3. Calculate the loss based on the difference between predictions and labels\n",
    "4. Backpropagation: calculate the gradients for every parameter with respect to the loss\n",
    "5. Update the parameters of the model in the direction of the gradients\n",
    "\n",
    "We have seen how we can do step 1, 2 and 4 in JAX and Flax. Now, we will look at step 3 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "For updating the parameters, Flax does not directly provide support for optimizers, but instead refers to another package called `optax` ([documentation](https://optax.readthedocs.io/en/latest/index.html)). Optax is an optimization library for JAX, which offers most common deep learning optimizers (SGD, Adam, Adagrad, RMSProp, etc.) and utilities (gradient clipping, weight decay, etc.).\n",
    "\n",
    "We import it below.\n",
    "If the import fails, you can install it using \n",
    "\n",
    "    !pip install --quiet optax\n",
    "\n",
    "directly from this notebook. This should also work on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will use the simplest optimizer, namely `optax.sgd`. Stochastic Gradient Descent updates parameters by multiplying the gradients with a small constant, called step size or learning rate, and subtracting those from the parameters (hence minimizing the loss). Therefore, we slowly move towards the direction of minimizing the loss. A good default value of the learning rate for a small network as ours is 0.1. Remember that we again aim to write *functional code*. Hence, the optimizer does not take as input the parameters, but only the optimizer hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the optimizer are optimizer settings like learning rate\n",
    "optimizer = optax.sgd(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since JAX calculates gradients via *function transformations*, we do not have functions like `backward()`, `optimizer.step()` or `optimizer.backward()` as in PyTorch. Instead, an optimizer is a function on the parameters and gradients. To simplify this step and bundle important parts of the training procedure, Flax offers the `flax.training` package. As a first step, we can create a `TrainState` which bundles the parameters, the optimizer, and the forward step of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "model_state = train_state.TrainState.create(apply_fn=model.apply, # The function to be minimized\n",
    "                                            params=params,        # The variables to be optimized\n",
    "                                            tx=optimizer)         # An Optax gradient transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this state object, it is easier to handle the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "For performing gradient updates, we need a function that can calculate the loss for a batch. Afterwards, we can apply JAX's gradient transformation to obtain a gradient function of it. In our setting, which is binary classification, we can use Binary Cross Entropy (BCE) which is defined as follows:\n",
    "\n",
    "$$\\mathcal{L}_{BCE} = -\\sum_i \\left[ y_i \\log x_i + (1 - y_i) \\log (1 - x_i) \\right]$$\n",
    "\n",
    "where $y$ are our labels, and $x$ our predictions, both in the range of $[0,1]$. Similar to PyTorch, Optax already provides a function for this: `optax.sigmoid_binary_cross_entropy(logits, labels)`. We calculate the loss on the logits instead of the sigmoid outputs for numerical stability. Let's write a function that takes as input a state (for the forward function), parameters, and a batch, and return the binary cross entropy loss and accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_acc(state, params, batch):\n",
    "    data_input, labels = batch\n",
    "    # Obtain the logits and predictions of the model for the input data\n",
    "    logits = state.apply_fn(params, data_input).squeeze(axis=-1)\n",
    "    pred_labels = (logits > 0).astype(jnp.float32)\n",
    "    # Calculate the loss and accuracy\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n",
    "    acc = (pred_labels == labels).mean()\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we explicitly add the parameters here as an input argument since we want to calculate the gradients with respect to them later. An example execution of the function would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "calculate_loss_acc(model_state, model_state.params, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an efficient training and validation step\n",
    "\n",
    "With this loss function and the optimizer, we are now ready to create an efficient training and validation/test step. First, let's consider the training. As input to each training step, we have a training state and a batch. We then want to calculate the loss for the input and take the gradients of it. Finally, we update the parameters with our optimizer and return the new state. All this can be summarized in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def train_step(state, batch):\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(calculate_loss_acc,  # Function to calculate the loss\n",
    "                                 argnums=1,  # Parameters are second argument of the function\n",
    "                                 has_aux=True  # Function has additional outputs, here accuracy\n",
    "                                )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, acc), grads = grad_fn(state, state.params, batch)\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, by using the transformation `jax.jit`, the whole gradient calculation and application is optimized in *XLA*, providing an efficient function for updating the model.\n",
    "\n",
    "Next, let's look at the evaluation function. Here, we do not need to calculate gradients, but only want to get the accuracy of the model for the batch. This becomes a simpler version of the training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    # Determine the accuracy\n",
    "    _, acc = calculate_loss_acc(state, state.params, batch)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions provide us now efficient utilities to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Finally, we are ready to train our model. As a first step, we create a slightly larger dataset and specify a data loader with a larger batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = XORDataset(size=2500, seed=42)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write a small training function. Depending on your available hardware, you do not need to explicitly push the model to GPU, since the parameters are already automatically created on GPU.\n",
    "Further, since the model itself is stateless, we do not have a `train()` or `eval()` function to switch between modes of e.g. dropout.\n",
    "When necessary, we can add an argument `train : bool` to the model forward pass.\n",
    "For this simple network here, however, this is not necessary.\n",
    "\n",
    "Let's write a function that trains a model for several epochs.\n",
    "The module `tqdm` provides a function called `tqdm` which makes printing of the progress of a loop much smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(state, data_loader, num_epochs=100):\n",
    "    # Training loop\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        for batch in data_loader:\n",
    "            state, loss, acc = train_step(state, batch)\n",
    "            # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "            # For simplicity, we skip this part here\n",
    "            pbar.set_description(f'Accuracy: {acc:4.2f}')\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_state = train_model(model_state, train_data_loader, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model for 100 epochs does take only some seconds...\n",
    "**Task**. You should also compare the runtime for the model when you comment the line `@jax.jit` before `def train_step(state, batch).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Once we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = XORDataset(size=500, seed=123)\n",
    "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
    "test_data_loader = data.DataLoader(test_dataset, \n",
    "                                   batch_size=128, \n",
    "                                   shuffle=False, \n",
    "                                   drop_last=False, \n",
    "                                   collate_fn=numpy_collate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our `eval_step` function to efficiently evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(state, data_loader):\n",
    "    all_accs, batch_sizes = [], []\n",
    "    for batch in data_loader:\n",
    "        batch_acc = eval_step(state, batch)\n",
    "        all_accs.append(batch_acc)\n",
    "        batch_sizes.append(batch[0].shape[0])\n",
    "    # Weighted average since some batches might be smaller\n",
    "    acc = sum([a*b for a,b in zip(all_accs, batch_sizes)]) / sum(batch_sizes)\n",
    "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(trained_model_state, test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we trained our model correctly, we should see a score close to 100% accuracy. However, this is only possible because of our simple task, and unfortunately, we usually don't get such high scores on test sets of more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binding model parameters\n",
    "\n",
    "Once we have trained the model, we might want to do multiple application of the same model and parameters. It can get a bit annoying to always write `model.apply(params, ...)` and keep track of the model and parameters separately. To prevent this, Flax's module can be bound to specific parameters to simplify our application. Specifically, we can bind the instance `model` of our `SimpleClassifier` class to our trained parameter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.bind(trained_model_state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model being binded to the parameters, we can use it as we would any PyTorch module. For instance, to apply it to an input array, we can simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input, labels = next(iter(data_loader))\n",
    "out = trained_model(data_input)  # No explicit parameter passing necessary anymore\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can simplify the analysis of models, and provide a more familiar interface to PyTorch users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing classification boundaries\n",
    "\n",
    "To visualize what our model has learned, we can perform a prediction for every data point in a range of $[-0.5, 1.5]$, and visualize the predicted class as in the sample figure at the beginning of this section. This shows where the model has created decision boundaries, and which points would be classified as $0$, and which as $1$. We therefore get a background image out of blue (class 0) and orange (class 1). The spots where the model is uncertain we will see a blurry overlap. The specific code is less relevant compared to the output figure which should hopefully show us a clear separation of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "def visualize_classification(model, data, label):\n",
    "    data_0 = data[label == 0]\n",
    "    data_1 = data[label == 1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(4,4), dpi=500)\n",
    "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
    "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
    "    plt.title(\"Dataset samples\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Let's make use of a lot of operations we have learned above\n",
    "    c0 = np.array(to_rgba(\"C0\"))\n",
    "    c1 = np.array(to_rgba(\"C1\"))\n",
    "    x1 = jnp.arange(-0.5, 1.5, step=0.01)\n",
    "    x2 = jnp.arange(-0.5, 1.5, step=0.01)\n",
    "    xx1, xx2 = jnp.meshgrid(x1, x2, indexing='ij')  # Meshgrid function as in numpy\n",
    "    model_inputs = np.stack([xx1, xx2], axis=-1)\n",
    "    logits = model(model_inputs)\n",
    "    preds = nn.sigmoid(logits)\n",
    "    output_image = (1 - preds) * c0[None,None] + preds * c1[None,None]  # Specifying \"None\" in a dimension creates a new one\n",
    "    output_image = jax.device_get(output_image)  # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n",
    "    plt.imshow(output_image, origin='lower', extent=(-0.5, 1.5, -0.5, 1.5))\n",
    "    plt.grid(False)\n",
    "    return fig\n",
    "\n",
    "_ = visualize_classification(trained_model, dataset.x, dataset.y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This concludes our tutorial on training a neural network with JAX. While the functional programming perspective of JAX may seem very different to PyTorch at first, it enables a considerable speedup in training, not only for tiny models like here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Operator network architecture\n",
    "\n",
    "**Task**. Consider a dataset with two kinds of features $y \\in \\mathbb{R}^{n}$ and $u \\in \\mathbb{R}^{m}$ and a scalar-valued output variable $z \\in \\mathbb{R}$.\n",
    "Create a neural neural network architecture which has the following architecture. Note that this architecture is used for [Deep Operator Learning](https://arxiv.org/abs/1910.03193).\n",
    "\n",
    "<center width=\"100%\"><img src=\"./fig_onet.png\" alt=\"Feedforward network\" width=\"500px\"></center>\n",
    "\n",
    "Use simple MLP networks for the branch and trunk net, resp. These networks map $y$ and $u$ to temporary outputs in $\\mathbb{R}^p$ which are then multiplied and summed to yield the scalar-valued output of the whole network.\n",
    "\n",
    "You can test your method on behalf of the example considered in **Part B**, although you should increase the size of your data set. It is a good exercise to create a PyTorch dataset from the data and to use the data loader and training procedure presented in **Part C**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here\n",
    "# Here, you should definitely add new markdown and code cells to improve the readability of the codes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
