{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe6d55f",
   "metadata": {},
   "source": [
    "# Hierarchical Agglomerative Clustering (HAC)\n",
    "\n",
    "**Goal:** Build a hierarchy of nested partitions of the dataset, from individual points to one all-encompassing cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm Steps\n",
    "\n",
    "### 1. Initialization\n",
    "- Start with the finest partition: each of the *n* data points is its own cluster.  \n",
    "- At this stage, the cluster distance matrix equals the point distance matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Find nearest clusters\n",
    "- Identify two distinct clusters with the minimal distance.  \n",
    "- That distance value is often called the **height of the merge**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Merge clusters\n",
    "- Combine the two closest clusters into a new cluster.  \n",
    "- This creates a new partition with one fewer cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Update cluster distances\n",
    "Recalculate the distances between the newly formed cluster and all the other clusters.  \n",
    "Different linkage methods determine how this update is done:\n",
    "\n",
    "- **Single linkage:** minimum distance between members.  \n",
    "- **Complete linkage:** maximum distance between members.  \n",
    "- **Average linkage:** average distance between members.  \n",
    "- **Ward’s method:** increase in total within-cluster variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Iterate\n",
    "- Repeat steps **2–4** until only one cluster remains (the coarsest partition).\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
