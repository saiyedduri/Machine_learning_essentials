{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9285c88c",
   "metadata": {},
   "source": [
    "Message passing interface(MPI) is primarily designed for distributed memory machines, though it can be useful in shared memory systems.It was first standardized in 1994.\n",
    "\n",
    "The MPI paradigm:\n",
    "    Processes can access local memory only but can communicate using messages.\n",
    "\n",
    "    The messages are passed from the local memory of one processor to the local memory of another processor magnitudes slower than to local memory.\n",
    "\n",
    "Latency transfer using MPI:    \n",
    " Latency infiband<0.6 µs, max 15m cable.\n",
    "    Bandwidth infiniband 400Gbit/s\n",
    "\n",
    "Advantages of MPI:\n",
    "1. Message passing in shared memory:\n",
    "    Using MPI in distributed and shared systems, it enables the programmer to manage data locality- by exactly specifying where the data goes. In shared distributed systems, message passing often means rewriting to another part of shared memory, so you dont pay a network cost.\n",
    "2. Debugging/overwriting advantage:\n",
    "     MPI communication is explicit(using MPI_Send,MPI_Recv etc), makig it easier to trace where the data is coming from and going to, making debugging less painful(compared to open_mp).\n",
    "3. MPI is a library, not a language:\n",
    "    MPI provides functions you call inside your program- its not a new programming language.\n",
    "    Bindings exist for C, C++, Python making it flexible.\n",
    "    There are open-sourced MPI implementations (like MPICH, OpenMPI, LAM/MPI) and also vendor tuned versions optimized for supercomputers.\n",
    "4. MPI is Portable:\n",
    "    Since MPI is a standard, not just an implementation, you can move MPI programs between systems pretty easily.\n",
    "5. Processes, not Threads:\n",
    "    MPI programs are made up of processes that talk via messages.\n",
    "    Processes can technically all run on the same machine, but that’s inefficient — you usually want them distributed across nodes in a cluster.\n",
    "\n",
    "Communicator:\n",
    "A group of processes communicating using MPI is defined using communicator. The communicator containing all processes is MPI_COMM_WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177efdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
