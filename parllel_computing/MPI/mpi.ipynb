{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9285c88c",
   "metadata": {},
   "source": [
    "Message passing interface(MPI) is primarily designed for distributed memory machines, though it can be useful in shared memory systems.It was first standardized in 1994.\n",
    "\n",
    "The MPI paradigm:\n",
    "    Processes can access local memory only but can communicate using messages.\n",
    "\n",
    "    The messages are passed from the local memory of one processor to the local memory of another processor magnitudes slower than to local memory.\n",
    "\n",
    "Latency transfer using MPI:    \n",
    " Latency infiband<0.6 µs, max 15m cable.\n",
    "    Bandwidth infiniband 400Gbit/s\n",
    "\n",
    "Advantages of MPI:\n",
    "1. Message passing in shared memory:\n",
    "    Using MPI in distributed and shared systems, it enables the programmer to manage data locality- by exactly specifying where the data goes. In shared distributed systems, message passing often means rewriting to another part of shared memory, so you dont pay a network cost.\n",
    "2. Debugging/overwriting advantage:\n",
    "     MPI communication is explicit(using MPI_Send,MPI_Recv etc), makig it easier to trace where the data is coming from and going to, making debugging less painful(compared to open_mp).\n",
    "3. MPI is a library, not a language:\n",
    "    MPI provides functions you call inside your program- its not a new programming language.\n",
    "    Bindings exist for C, C++, Python making it flexible.\n",
    "    There are open-sourced MPI implementations (like MPICH, OpenMPI, LAM/MPI) and also vendor tuned versions optimized for supercomputers.\n",
    "4. MPI is Portable:\n",
    "    Since MPI is a standard, not just an implementation, you can move MPI programs between systems pretty easily.\n",
    "5. Processes, not Threads:\n",
    "    MPI programs are made up of processes that talk via messages.\n",
    "    Processes can technically all run on the same machine, but that’s inefficient — you usually want them distributed across nodes in a cluster.\n",
    "\n",
    "## Communicator:\n",
    "In MPI, a communicator is an object that defines a group of processes that can communicate with each other. \n",
    "MPI.COMM_WORLD\n",
    "\n",
    "## Rank:\n",
    "- rank is the unique ID (an integer) of the process within COMM_WORLD.\n",
    "- It starts at 0 and goes up to size - 1.\n",
    "\n",
    "- Example: If you run the program with 4 processes, the ranks will be: 0, 1, 2, 3.\n",
    "\n",
    "## size:\n",
    "- size is the total number of processes in COMM_WORLD.\n",
    "\n",
    "-This tells each process how many other processes are running.\n",
    "\n",
    "- Example: If you start the program with 4 processes, each will see size == 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f12d5",
   "metadata": {},
   "source": [
    "# print_rank_size.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "# MPI.COMM_WORLD is the default communicator \n",
    "comm=MPI.COMM_WORLD\n",
    "\n",
    "# rank is the unique ID of the process within COMM_WORLD. It starts from 0 to size-1\n",
    "rank=comm.Get_rank()\n",
    "size=comm.Get_size()\n",
    "\n",
    "print(\"Hello World!\")\n",
    "print(f\"rank={rank}, size={size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337dde0",
   "metadata": {},
   "source": [
    "# Broadcast in mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d635c",
   "metadata": {},
   "source": [
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "def main(argv):\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "\n",
    "    print(\"Hello world!\")\n",
    "    print(f\"rank={rank} size={size}\")\n",
    "\n",
    "    comm.Barrier()  # Every process stops here and waits untill all process arrrive and then move forward together.\n",
    "\n",
    "    if rank == 0:\n",
    "        n = (rank + 1) * 4711\n",
    "    else:\n",
    "        n = None\n",
    "\n",
    "    n = comm.bcast(n, root=0)  # broadcast from rank 0 to remaining processes.\n",
    "\n",
    "    print(f\"Rank {rank} received = {n}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n",
    "    MPI.Finalize() # Shutsdown all the MPI processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743789",
   "metadata": {},
   "source": [
    "# output:\n",
    "(base) [sy37tovi@mlogin01 mpi]$ mpiexec -n 5 python Bcast.py\n",
    "Hello world!\n",
    "rank=3; size=5\n",
    "Hello world!\n",
    "rank=1; size=5\n",
    "Hello world!\n",
    "Hello world!\n",
    "rank=4; size=5\n",
    "rank=2; size=5\n",
    "Hello world!\n",
    "rank=0; size=5\n",
    "Rank 0 received = 4711\n",
    "Rank 2 received = 4711\n",
    "Rank 4 received = 4711\n",
    "Rank 3 received = 4711\n",
    "Rank 1 received = 4711"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a639a",
   "metadata": {},
   "source": [
    "# Conventions present in mpi4py\n",
    "The mpi4py library uses lower case (comm.bcast) for MPI communication of Python objects(convenient, high level - but lower case) and \n",
    "upper case (comm.Bcast) for buffers/arrays (low level, potentially faster).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d3f0f",
   "metadata": {},
   "source": [
    "Example:\n",
    "from mpi4py import MPI\n",
    "def main():\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "root = 0\n",
    "# Using comm.bcast (Python object; an int is an object in Python)\n",
    "if rank == root:\n",
    "    value = 42\n",
    "    else:\n",
    "    value = None\n",
    "value = comm.bcast(value, root=root)\n",
    "print(f\"Rank {rank}: Received value using comm.bcast: {value}\")\n",
    "\n",
    "\n",
    "\n",
    "# Using comm.Bcast (numpy array)\n",
    "import numpy as np\n",
    "if rank == root:\n",
    "    value_np = np.array([42], dtype='i')\n",
    "else:\n",
    "    value_np = np.empty(1, dtype='i')\n",
    "comm.Bcast([value_np, MPI.INT], root=root) #This is a buffer specification in mpi4py’s lower-level interface.  The MPI datatype (here: 32-bit integer). This tells MPI how to                                   interpret the raw bytes in the buffer.By passing this list, you’re giving MPI direct access to the array’s\n",
    "print(f\"Rank {rank}: Received value using comm.Bcast: {value_np[0]}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f533e4e",
   "metadata": {},
   "source": [
    "# MPI Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a44e3",
   "metadata": {},
   "source": [
    "An MPI reduction is an operation where values from all ranks are combined into a single result, usually at one “root” rank, using an operation like sum, max, min, product, logical AND/OR, etc.\n",
    "\n",
    "Common MPI reduction operations\n",
    "MPI.SUM → sum of values\n",
    "\n",
    "MPI.PROD → product of values\n",
    "\n",
    "MPI.MAX → maximum value\n",
    "\n",
    "MPI.MIN → minimum value\n",
    "\n",
    "MPI.LAND → logical AND\n",
    "\n",
    "MPI.LOR → logical OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ab79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "def main(argv):\n",
    "        comm=MPI.COMM_WORLD\n",
    "        rank=comm.Get_rank()\n",
    "        size=comm.Get_size()\n",
    "        print(\"Hello world!\")\n",
    "        print(f\"rank={rank} of size={size}\")\n",
    "        comm.Barrier()\n",
    "\n",
    "        n=rank+1\n",
    "        prod=comm.reduce(n,op=MPI.PROD,root=0)\n",
    "\n",
    "        if rank==0:\n",
    "                print(f\"received={prod}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "        main(sys.argv)\n",
    "        MPI.Finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973d021",
   "metadata": {},
   "source": [
    "Hello world!\n",
    "rank=2 of size=5\n",
    "Hello world!\n",
    "rank=3 of size=5\n",
    "Hello world!\n",
    "rank=4 of size=5\n",
    "Hello world!\n",
    "rank=0 of size=5\n",
    "Hello world!\n",
    "rank=1 of size=5\n",
    "received=120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed090cc0",
   "metadata": {},
   "source": [
    " allreduce — similar to reduce, but every rank gets the result instead of only the root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea2abf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c828e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    print(\"Hello world!\")\n",
    "    print(f\"rank={rank} size={size}\")\n",
    "    n = rank + 1\n",
    "    comm.Barrier()\n",
    "    # MPI_Reduce\n",
    "    sum = comm.reduce(n, op=MPI.SUM, root=0)\n",
    "    if rank == 0:\n",
    "        print(f\" Received reduce={sum}\")\n",
    "    comm.Barrier()\n",
    "    # MPI_Allreduce\n",
    "    sum_allreduce = comm.allreduce(n, op=MPI.SUM)\n",
    "    print(f\" Received Allreduce ={sum_allreduce}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n",
    "    MPI.Finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947505a9",
   "metadata": {},
   "source": [
    "(base) [sy37tovi@mlogin01 mpi]$ mpiexec -n 5 python all_reduce.py\n",
    "Hello world!\n",
    "rank=3; size=5\n",
    "Hello world!\n",
    "rank=1; size=5\n",
    "Hello world!\n",
    "rank=0; size=5\n",
    "Hello world!\n",
    "rank=4; size=5\n",
    "Hello world!\n",
    "rank=2; size=5\n",
    "received reduce=15\n",
    "Received allreduce=15\n",
    "Received allreduce=15\n",
    "Received allreduce=15\n",
    "Received allreduce=15\n",
    "Received allreduce=15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4c4ac",
   "metadata": {},
   "source": [
    "# Measuring Walltime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm=MPI.COMM_WORLD\n",
    "\n",
    "comm.Barrier()\n",
    "starttime=MPI.Wtime()\n",
    "\n",
    "rank=comm.Get_rank()\n",
    "size=comm.Get_size()\n",
    "\n",
    "print(\"Hello world!\")\n",
    "print(f\"rank={rank},size={size}\")\n",
    "\n",
    "comm.Barrier()\n",
    "endtime=MPI.Wtime()\n",
    "\n",
    "elapsed_time=endtime-starttime\n",
    "\n",
    "sum=comm.reduce(elapsed_time,op=MPI.SUM,root=0)\n",
    "\n",
    "if rank==0:\n",
    "        print(f\"Sum of elapsed time: {sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66aa3",
   "metadata": {},
   "source": [
    "(base) [sy37tovi@mlogin01 mpi]$ mpiexec -n 5 python mpi.py\n",
    "Hello world!\n",
    "rank=2,size=5\n",
    "Hello world!\n",
    "rank=3,size=5\n",
    "Hello world!\n",
    "rank=4,size=5\n",
    "Hello world!\n",
    "rank=0,size=5\n",
    "Hello world!\n",
    "rank=1,size=5\n",
    "Sum of elapsed time: 0.00014458100000000002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08516c",
   "metadata": {},
   "source": [
    "# MPI_Send\n",
    "MPI_Send lets one rank (process) send a message (data buffer) to another rank.\n",
    "\n",
    "It’s a blocking send, meaning the sending process might wait until the message data has been copied out (depending on MPI implementation).\n",
    "\n",
    "You specify:\n",
    "\n",
    "    - The data buffer to send\n",
    "\n",
    "    - The destination rank (which process to send to)\n",
    "\n",
    "    - A message tag (an integer to help match sends and receives)\n",
    "\n",
    "    - The communicator (usually MPI.COMM_WORLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2af4c",
   "metadata": {},
   "source": [
    "# comm.send:\n",
    "Sends buf from calling process to dest. The integer value tag is an ID from 0...32767(minimum guaranteed by the standard).\n",
    "\n",
    "Python:\n",
    "comm.Send(buf, dest=dest, tag=tag)s\n",
    "MPI_Send lets one rank (process) send a message (data buffer) to another rank.\n",
    "\n",
    "It’s a blocking send, meaning the sending process might wait until the message data has been copied out (depending on MPI implementation).\n",
    "\n",
    "You specify:\n",
    "\n",
    "The data buffer to send\n",
    "\n",
    "The destination rank (which process to send to)\n",
    "\n",
    "A message tag (an integer to help match sends and receives)\n",
    "\n",
    "The communicator (usually MPI.COMM_WORLD)\n",
    "\n",
    "# comm.recv:\n",
    "\n",
    "MPI_Recv lets one rank receive a message from another rank.\n",
    "\n",
    "It is blocking — the receiving process waits until the message arrives.\n",
    "\n",
    "You specify:\n",
    "\n",
    "A buffer to store the incoming data\n",
    "\n",
    "The source rank (which process to receive from; or MPI.ANY_SOURCE to accept from anyone)\n",
    "\n",
    "The message tag to match the send"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b96fd6",
   "metadata": {},
   "source": [
    "# What happens during send?\n",
    "\n",
    "# Handshaking with the receiver\n",
    "MPI might first contact the destination process to ensure it is ready to receive (MPI_Recv posted).\n",
    "\n",
    "For synchronous sends (like MPI_Ssend), your process waits until the receiver actually starts receiving.\n",
    "\n",
    "\n",
    "# Copying data into MPI’s internal buffer (if available)\n",
    "If the receiver isn’t ready, MPI might copy your send buffer into an internal temporary buffer so you can reuse your array.\n",
    "\n",
    "If there’s no buffer space available (common in large messages), you wait until the receiver is ready.\n",
    "\n",
    "# Actual network transfer\n",
    "For large messages, MPI may require the data to be completely transferred before returning.\n",
    "\n",
    "This can involve:\n",
    "\n",
    "Writing to shared memory (if sender/receiver are on the same node)\n",
    "\n",
    "Sending packets across network hardware (InfiniBand, Ethernet, etc.)\n",
    "\n",
    "Waiting for acknowledgments from the destination\n",
    "\n",
    "\n",
    "While you’re “waiting” in MPI_Send, your process can’t do anything else — MPI is busy completing enough of the send operation to make your buffer safe, which may involve:\n",
    "\n",
    "Waiting for the receiver\n",
    "\n",
    "Copying to MPI buffers\n",
    "\n",
    "Fully transmitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data0 = \"Hello from rank 0\"\n",
    "    comm.send(data0, dest=2, tag=11)  # send string to rank 1\n",
    "elif rank == 1:\n",
    "    data1 = \"Hello from rank 0\"\n",
    "    comm.send(data1, dest=2, tag=12)  # send string to rank 1\n",
    "elif rank==2:\n",
    "    data0=comm.recv(source=MPI.ANY_SOURCE,tag=11) #receive string from rank 0   \n",
    "    data1=comm.recv(source=MPI.ANY_SOURCE,tag=12) #receive string from rank 1   \n",
    "    print(f\"Rank1 received: {data0, data1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d143d70d",
   "metadata": {},
   "source": [
    "(base) [sy37tovi@mlogin01 mpi]$ mpiexec -n 5 python mpi_send.py\n",
    "Rank1 received: ('Hello from rank 0', 'Hello from rank 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53618da6",
   "metadata": {},
   "source": [
    "# Difference b/w MPI_send and MPI_Isend\n",
    "\n",
    "# Blocking MPI_send:\n",
    "\n",
    "The function doesn’t return control to your program until the operation is complete enough for MPI to guarantee safety.\n",
    "\n",
    "Your process is “stuck” inside the MPI library during that time — it’s not running your code, not computing, not doing anything else.\n",
    "\n",
    "Standard Mode Send\n",
    "Mode: Standard send (MPI chooses between buffered or synchronous internally).\n",
    "Behavior:\n",
    "\n",
    "May complete before the matching receive is posted if MPI can copy the message into an internal buffer.\n",
    "\n",
    "May also block until the receiver starts receiving, if no buffer is available or the message is large\n",
    "When send returns: The send buffer is safe to reuse, but no guarantee the receiver has started reading it yet.\n",
    "\n",
    "# MPI_Send — Standard Mode Send\n",
    "Mode: Standard send (MPI chooses between buffered or synchronous internally).\n",
    "\n",
    "Behavior:\n",
    "May complete before the matching receive is posted if MPI can copy the message into an internal buffer.\n",
    "\n",
    "May also block until the receiver starts receiving, if no buffer is available or the message is large.\n",
    "\n",
    "When send returns: The send buffer is safe to reuse, but no guarantee the receiver has started reading it yet.\n",
    "\n",
    "Example:\n",
    "\n",
    "# MPI_Ssend — Synchronous Mode Send\n",
    "Mode: Synchronous send (always handshake with the receiver).\n",
    "\n",
    "Behavior:\n",
    "Never completes until the matching receive has started.\n",
    "\n",
    "Requires a rendezvous protocol: sender and receiver exchange a handshake before data transfer.\n",
    "\n",
    "When send returns: The send buffer is safe to reuse and you are guaranteed the receiver has entered the receive call for that message.\n",
    "\n",
    "Here’s how it recognizes it:\n",
    "\n",
    "When a process calls Recv, it tells MPI “I’m ready to receive a message from this source.”\n",
    "\n",
    "MPI matches this receive with the corresponding Ssend waiting on the sender side.\n",
    "\n",
    "Once the receive is posted, MPI allows the Ssend to proceed and complete.\n",
    "\n",
    "| Feature                    | `MPI_Send` (Standard)                     | `MPI_Ssend` (Synchronous)                   |\n",
    "| -------------------------- | ----------------------------------------- | ------------------------------------------- |\n",
    "| Completion condition       | Buffer copied or receiver started receive | Receiver has started receive (handshake)    |\n",
    "| May return before receive? | Yes (if buffered)                         | No                                          |\n",
    "| Guarantee about receiver?  | None                                      | Yes — receiver has posted the matching recv |\n",
    "| Potential to block long?   | Yes, but depends on buffers               | Yes — will block until recv starts          |\n",
    "| Typical use                | General communication                     | Debugging ordering or avoiding buffering    |\n",
    "\n",
    "\n",
    "# Non-blocking MPI_Isend:\n",
    "\n",
    "Non-blocking: Initiates the send but returns immediately, even before data is copied into MPI’s internal buffer (if any).\n",
    "\n",
    "You cannot reuse or modify the send buffer until you call MPI_Wait or MPI_Test and confirm completion.\n",
    "\n",
    "Gives you overlap of computation and communication — you can start sending and do other work before ensuring completion.\n",
    "\n",
    "Still uses the standard send mode internally, so the same buffering/synchronous behavior applies — the difference is that progress happens in the background.\n",
    "\n",
    "| Feature                     | `MPI.Send` (Blocking)                           | `MPI.Isend` (Non-blocking)                               |\n",
    "| --------------------------- | ----------------------------------------------- | -------------------------------------------------------- |\n",
    "| Returns immediately?        | No (waits until buffer safe to reuse)           | Yes (but you must wait/test later before reusing buffer) |\n",
    "| Mode under the hood         | Standard (impl chooses buffered or synchronous) | Standard (same)                                          |\n",
    "| Can overlap compute & comm? | No                                              | Yes                                                      |\n",
    "| Need `MPI_Wait`/`MPI_Test`? | No                                              | Yes                                                      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2d04f",
   "metadata": {},
   "source": [
    "# Deadlock:\n",
    "A deadlock in MPI (and in programming in general) happens when two or more processes are each waiting for the other to do something, so nobody can move forward — the program just gets stuck forever.\n",
    "\n",
    "| Time | Rank 0        | Rank 1        |\n",
    "| ---- | ------------- | ------------- |\n",
    "| t0   | Send → wait   | Send → wait   |\n",
    "| t1(Deadlock)   | waiting…      | waiting…      |\n",
    "| t2   | still waiting | still waiting |\n",
    "\n",
    "Here’s what happens:\n",
    "\n",
    "Rank 0 starts Send to Rank 1 — it waits for Rank 1 to post a matching Recv.\n",
    "\n",
    "Rank 1 starts Send to Rank 0 — it waits for Rank 0 to post a matching Recv.\n",
    "\n",
    "Both are waiting for each other to post a receive… but no one ever does.\n",
    "→ Deadlock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66dd8f",
   "metadata": {},
   "source": [
    "# b) Producing a deadlock with MPI.Ssend\n",
    "If every process tries to send first using Ssend before posting a recv, all will block waiting for the receiver to post a matching receive - but no one ever does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm=MPI.COMM_WORLD\n",
    "rank=comm.Get_rank()\n",
    "size=comm.Get_size()\n",
    "\n",
    "send_data=rank\n",
    "recv_data= -1\n",
    "next_rank=(rank+1)%size\n",
    "prev_rank=(rank-1)%size\n",
    "\n",
    "comm.Ssend(send_data,dest=next_rank)\n",
    "comm.Recv(recv_data,source=prev_rank)\n",
    "\n",
    "print(f\"Rank {rank} received {recv_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158e45b",
   "metadata": {},
   "source": [
    "# (c) Avoiding the deadlock\n",
    "Two common fixes:\n",
    "\n",
    "Post the Recv before the Ssend (so receiver is ready):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm.Recv(recv_data, source=prev_rank)\n",
    "comm.Ssend(send_data, dest=next_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269484a",
   "metadata": {},
   "source": [
    "# (d) MPI.Isend and MPI.Recv\n",
    "MPI.Isend → Immediate (non-blocking) send:\n",
    "\n",
    "Returns immediately after initiating the send, before the receiver is ready.\n",
    "\n",
    "The send completes later; you must call .Wait() or .Test() on the returned request to ensure completion.\n",
    "\n",
    "Often used to overlap computation with communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfdc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = comm.Isend(send_data, dest=next_rank)\n",
    "comm.Recv(recv_data, source=prev_rank)\n",
    "req.Wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b8200",
   "metadata": {},
   "source": [
    "# (e) Isend + Recv vs Send + Irecv\n",
    "Isend + Recv:\n",
    "\n",
    "Send is non-blocking, receive is blocking.\n",
    "\n",
    "Good if you want to start sending early and only wait after receiving.\n",
    "\n",
    "Send + Irecv:\n",
    "\n",
    "Send is blocking, receive is non-blocking.\n",
    "\n",
    "Good if you want to post a receive early and process data when it arrives without blocking.\n",
    "\n",
    "Key difference: Which direction of communication is allowed to progress without waiting — sending or receiving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3707ed0",
   "metadata": {},
   "source": [
    "Why MPI_Irecv is useful\n",
    "\n",
    "1. Overlap computation & communication\n",
    "You don’t have to wait at the recv call — computation can happen while data is moving.\n",
    "\n",
    "2. Avoid deadlocks\n",
    "If two processes both call MPI_Send first (blocking), each will wait for the other to post a receive — deadlock.\n",
    "With MPI_Isend/MPI_Irecv, both can start the exchange without blocking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da882295",
   "metadata": {},
   "source": [
    "# MPI_Sendrecv\n",
    "MPI_Sendrecv is a combined blocking operation that sends a message to one process and simultaneously receives a message from another (or the same) process.\n",
    "\n",
    "It avoids deadlocks that can happen when you call MPI_Send and MPI_Recv separately, especially when two processes are sending and receiving data from each other at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9080dc",
   "metadata": {},
   "source": [
    "# MPI_Alltoall\n",
    "MPI_Alltoall is a collective communication operation where every process sends data to every other process, including itself, and receives data from every other process as well.\n",
    "\n",
    "It’s like a “full exchange” of data between all ranks in the communicator.\n",
    "\n",
    "Imagine you have N processes (like workers in a group). Each worker has N pieces of information, one piece meant for each worker (including themselves).\n",
    "\n",
    "When MPI_Alltoall runs, each worker sends the piece meant for worker 0 to worker 0, the piece meant for worker 1 to worker 1, and so on — all at once.\n",
    "\n",
    "At the end, every worker has received one piece from every other worker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea9be1",
   "metadata": {},
   "source": [
    "# Splitting Communicators:\n",
    "\n",
    "Splitting a communicator means:\n",
    "\n",
    "    Start with a big group of processes (oldcomm, often MPI.COMM_WORLD).\n",
    "\n",
    "    Assign each process a color (integer).\n",
    "\n",
    "    Same color → goes into the same subgroup.\n",
    "\n",
    "    Different color → goes into different subgroups.\n",
    "\n",
    "Inside each subgroup, order processes by key (smallest key = rank 0 in the new subgroup).\n",
    "\n",
    "The result is a new communicator for each subgroup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce05cd2",
   "metadata": {},
   "source": [
    "# MPI_Gather\n",
    "\n",
    "MPI_Gather collects the data from all processors in a communicator and gathers it into a single process called the root.\n",
    "\n",
    "Each process sends its local data to the root process.\n",
    "\n",
    "The root process receives all the data and stores it in a buffer (usually an array or list)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e34f96",
   "metadata": {},
   "source": [
    "# MPI_Scatter\n",
    "\n",
    "MPI_Scatter distributes chunks of data from the root process to all other processes in the communicator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21fd9d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93dce12c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2865953f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
