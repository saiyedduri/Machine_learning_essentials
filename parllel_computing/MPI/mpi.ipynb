{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9285c88c",
   "metadata": {},
   "source": [
    "Message passing interface(MPI) is primarily designed for distributed memory machines, though it can be useful in shared memory systems.It was first standardized in 1994.\n",
    "\n",
    "The MPI paradigm:\n",
    "    Processes can access local memory only but can communicate using messages.\n",
    "\n",
    "    The messages are passed from the local memory of one processor to the local memory of another processor magnitudes slower than to local memory.\n",
    "\n",
    "Latency transfer using MPI:    \n",
    "- Latency infiband<0.6 µs, max 15m cable.\n",
    "- Bandwidth infiniband 400Gbit/s\n",
    "\n",
    "Advantages of MPI:\n",
    "1. Message passing in shared memory:\n",
    "    Using MPI in distributed and shared systems, it enables the programmer to manage data locality- by exactly specifying where the data goes. In shared distributed systems, message passing often means rewriting to another part of shared memory, so you dont pay a network cost.\n",
    "2. Debugging/overwriting advantage:\n",
    "     MPI communication is explicit(using MPI_Send,MPI_Recv etc), makig it easier to trace where the data is coming from and going to, making debugging less painful(compared to open_mp).\n",
    "3. MPI is a library, not a language:\n",
    "    MPI provides functions you call inside your program- its not a new programming language.\n",
    "    Bindings exist for C, C++, Python making it flexible.\n",
    "    There are open-sourced MPI implementations (like MPICH, OpenMPI, LAM/MPI) and also vendor tuned versions optimized for supercomputers.\n",
    "4. MPI is Portable:\n",
    "    Since MPI is a standard, not just an implementation, you can move MPI programs between systems pretty easily.\n",
    "5. Processes, not Threads:\n",
    "    MPI programs are made up of processes that talk via messages.\n",
    "    Processes can technically all run on the same machine, but that’s inefficient — you usually want them distributed across nodes in a cluster.\n",
    "\n",
    "To check number of nodes, cpu, and MPI processes in the cluster:\n",
    "\n",
    "qstat -Bf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21919d9",
   "metadata": {},
   "source": [
    "Communicator:\n",
    "- A group of processes communicating using MPI is defined using communicator. The communicator containing all processes is MPI_COMM_WORLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c151b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In print_rank_size/print_rank_size.py script:\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "# print the rank and size\n",
    "rank=comm.Get_rank()\n",
    "size=comm.Get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f6611",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#PBS -N combined_parllel_for_loop\n",
    "#PBS -q teachingq\n",
    "#PBS -l select=1:ncpus=4:mpiprocs=4\n",
    "#PBS -l walltime=00:01:00\n",
    "#PBS -o log.out2\n",
    "#PBS -e log.err2\n",
    "export OMP_NUM_THREADS=4\n",
    "\n",
    "echo -e \"Job started from $(pwd).\"\n",
    "echo \"Changing directory to...\"\n",
    "PBS_O_WORKDIR=/home/sy37tovi/parllel_computing2/MPI/mpi_examples\n",
    "cd \"$PBS_O_WORKDIR\"\n",
    "echo -e \"$(pwd)\"\n",
    "\n",
    "mpiexec -n 4 python print_rank_size.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c0ca7",
   "metadata": {},
   "source": [
    "# output:\n",
    "Hello World!\n",
    "\n",
    "rank=1, size=4\n",
    "\n",
    "Hello World!\n",
    "\n",
    "rank=2, size=4\n",
    "\n",
    "Hello World!\n",
    "\n",
    "rank=0, size=4\n",
    "\n",
    "Hello World!\n",
    "\n",
    "rank=3, size=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a608b4",
   "metadata": {},
   "source": [
    "# Rank: \n",
    "- The unique ID number assigned to each process in a communicator (typically MPI.COMM_WORLD).\n",
    "\n",
    "- Ranges from 0 to size - 1.\n",
    "\n",
    "- Used to differentiate processes and direct communication (e.g., “send data from rank 0 to rank 2”).\n",
    "\n",
    "# Size:\n",
    "- The total number of processes in a communicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a446cde",
   "metadata": {},
   "source": [
    "Use OpenMPI if:\n",
    "\n",
    "You want maximum hardware performance on Linux clusters.\n",
    "\n",
    "You're working with custom networks or need modular control.\n",
    "\n",
    "Use MPICH if:\n",
    "\n",
    "You care about portability and standard compliance.\n",
    "\n",
    "You're using Intel MPI, MS MPI, or a system that builds on MPICH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d84dd",
   "metadata": {},
   "source": [
    "# MPI_Bcast:\n",
    "MPI_Bcast is used to broadcast data from one process to all other processes in the communictaor\n",
    "- Only the root process provides the data\n",
    "- All processes (including the root) receive the same data\n",
    "- It's a way to share the same information across all processes efficiently.\n",
    "\n",
    "# MPI Broadcast:\n",
    "Broadcast the messaage in the buffer of the process with rank 'root' to all processes. Note that all processes call MPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8e2b5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#broadcast_list.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "list=[0,1,2,3]\n",
    "count=4\n",
    "comm=MPI.COMM_WORLD\n",
    "rank=comm.Get_rank()\n",
    "\n",
    "if rank!=0:\n",
    "        list=None\n",
    "\n",
    "print(\"data before broadcasting to other threads\")\n",
    "print(f\"received_data={list},rank={rank}\")\n",
    "received_data=comm.bcast(list,root=0)\n",
    "print(f\"received_data={received_data}; rank={rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa3348",
   "metadata": {},
   "source": [
    "# output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f95298",
   "metadata": {},
   "source": [
    "\n",
    "Job started from /home/sy37tovi/pbs.943649.mmaster02.x8z.\n",
    "\n",
    "Changing directory to...\n",
    "\n",
    "/home/sy37tovi/parllel_computing2/MPI/mpi_examples/broadcast_mpi\n",
    "\n",
    "data before broadcasting to other threads\n",
    "\n",
    "received_data=[0, 1, 2, 3],rank=0\n",
    "\n",
    "data before broadcasting to other threads\n",
    "\n",
    "received_data=None,rank=1\n",
    "\n",
    "data before broadcasting to other threads\n",
    "\n",
    "received_data=None,rank=2\n",
    "\n",
    "data before broadcasting to other threads\n",
    "\n",
    "received_data=None,rank=3\n",
    "\n",
    "received_data=[0, 1, 2, 3]; rank=0\n",
    "\n",
    "received_data=[0, 1, 2, 3]; rank=1\n",
    "\n",
    "received_data=[0, 1, 2, 3]; rank=2\n",
    "\n",
    "received_data=[0, 1, 2, 3]; rank=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d481b0",
   "metadata": {},
   "source": [
    "# Difference between broadcasting python objects and numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28330ff9",
   "metadata": {},
   "source": [
    "| Feature      | `comm.bcast()`                   | `comm.Bcast()`                                |\n",
    "| ------------ | -------------------------------- | --------------------------------------------- |\n",
    "| Abstraction  | High-level                       | Low-level                                     |\n",
    "| Data types   | Any Python object (via pickling) | Only buffer-like objects (e.g., NumPy arrays) |\n",
    "| Return value | Returns broadcasted object       | Returns `None` (in-place)                     |\n",
    "| Performance  | Slower (due to pickling)         | Faster (direct memory broadcast)              |\n",
    "| Use case     | Simpler, flexible usage          | Performance-critical code, large data         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910549b8",
   "metadata": {},
   "source": [
    "comm.Barrier()\n",
    "A synchronization point — all processes must reach it before any continue. This is often used to line up output or timing.\n",
    "\n",
    "comm.reduce(n, op=MPI.SUM, root=0)\n",
    "Each process sends its n to rank 0, which performs a sum reduction:\n",
    "\n",
    "If 4 ranks: 1 + 2 + 3 + 4 = 10\n",
    "\n",
    "Only rank 0 receives the result; others get None.\n",
    "\n",
    "MPI.Finalize(): \n",
    "Cleans up MPI — in mpi4py, this is optional because it finalizes automatically at exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import sys\n",
    "def main(argv):\n",
    "        comm=MPI.COMM_WORLD\n",
    "        rank=comm.Get_rank()\n",
    "        size=comm.Get_size()\n",
    "        print(\"Hello world!\")\n",
    "        print(f\"rank={rank}; size={size}\")\n",
    "        comm.Barrier()\n",
    "\n",
    "        n=rank+1\n",
    "        sum=comm.reduce(n,op=MPI.SUM,root=0)\n",
    "\n",
    "        if rank==0:\n",
    "                print(f\" Received total sum at rank {rank} ={sum}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "        main(sys.argv)\n",
    "        MPI.Finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de3b5b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
